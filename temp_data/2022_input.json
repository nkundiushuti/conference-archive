[
    {
        "title": "Singing beat tracking with Self-supervised front-end and linear transformers",
        "author": [
            "Mojtaba Heydari",
            "Zhiyao Duan"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "617-624",
        "abstract": "Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction.\n Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. \n Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies \n also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000074.pdf"
    },
    {
        "title": "Using Activation Functions for Improving Measure-Level Audio Synchronization",
        "author": [
            "Yigitcan Özer",
            "Matej Ištvánek",
            "Vlora Arifi-Müller",
            "Meinard Müller"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "749-755",
        "abstract": "Audio synchronization aims at aligning multiple recordings of the same piece of music. Traditional synchronization approaches are often based on dynamic time warping using chroma features as an input representation. Previous work has shown how one can integrate onset cues into this pipeline for improving the alignment's temporal accuracy. Furthermore, recent work based on deep neural networks has led to significant improvements for learning onset, beat, and downbeat activation functions. However, for music with soft onsets and abrupt tempo changes, these functions may be unreliable, leading to unstable results. As the main contribution of this paper, we introduce a combined approach that integrates activation functions into the synchronization pipeline. We show that this approach improves the temporal accuracy thanks to the activation cues while inheriting the robustness of the traditional synchronization approach. Conducting experiments based on string quartet recordings, we evaluate our combined approach where we transfer measure annotations from a reference recording to a target recording.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000090.pdf"
    },
    {
        "title": "Domain Adversarial Training on Conditional Variational Auto-Encoder for Controllable Music Generation",
        "author": [
            "Jingwei Zhao",
            "Gus Xia",
            "Ye Wang"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "925-932",
        "abstract": "The variational auto-encoder has become a leading framework for symbolic music generation, and a popular research direction is to study how to effectively control the generation process. A straightforward way is to control a model using different conditions during inference. However, in music practice, conditions are usually sequential (rather than simple categorical labels), involving rich information that overlaps with the learned representation. Consequently, the decoder gets confused about whether to \"listen to\" the latent representation or the condition, and sometimes just ignores the condition. To solve this problem, we leverage domain adversarial training to disentangle the representation from condition cues for better control. Specifically, we propose a condition corruption objective that uses the representation to denoise a corrupted condition. Minimized by a discriminator and maximized by the VAE encoder, this objective adversarially induces a condition-invariant representation. In this paper, we focus on the task of melody harmonization to illustrate our idea, while our methodology can be generalized to other controllable generative tasks. Demos and experiments show that our methodology facilitates not only condition-invariant representation learning but also higher-quality controllability compared to baselines.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000111.pdf"
    }
]
