[
 {
        "title": "Multi-pitch Estimation meets Microphone Mismatch: Applicability of Domain Adaptation",
        "author": [
            "Franca Bittner",
            "Marcel Gonzalez",
            "Maike L Richter",
            "Hanna Lukashevich",
            "Jakob Abeßer"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "477-484",
        "abstract": "The performance of machine learning (ML) models is known to be affected by discrepancies between training (source) and real-world (target) data distributions. This problem is referred to as domain shift and is commonly approached using domain adaptation (DA) methods. As one relevant scenario, automatic piano transcription algorithms in music learning applications potentially suffer from domain shift since pianos are recorded in different acoustic conditions using various devices. Yet, most currently available datasets for piano transcription only cover ideal recording situations with high-quality microphones. Consequently, a transcription model trained on these datasets will face a mismatch between source and target data in real-world scenarios. To address this issue, we employ a recently proposed dataset which includes annotated piano recordings covering typical real-life recording settings for a piano learning application on mobile devices. We first quantify the influence of the domain shift on the performance of a deep learning-based piano multi-pitch estimation (MPE) algorithm. Then, we employ and evaluate four unsupervised DA methods to reduce domain shift. Our results show that the studied MPE model is surprisingly robust to domain shift in microphone mismatch scenarios and the DA methods do not notably improve the transcription performance.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000057.pdf"
    },
    {
        "title": "Melody transcription via generative pre-training",
        "author": [
            "Chris Donahue",
            "John Thickstun",
            "Percy Liang"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "485-492",
        "abstract": "Despite the central role that melody plays in music perception, it remains an open challenge in MIR to reliably detect the notes of the melody present in an arbitrary music recording. A key challenge in *melody transcription* is building methods which can handle broad audio containing any number of instrument ensembles and musical styles---existing strategies work well for some melody instruments or styles but not all. To confront this challenge, we leverage representations from Jukebox (Dhariwal et al. 2020), a generative model of broad music audio, thereby improving performance on melody transcription by 20% relative to conventional spectrogram features. Another obstacle in melody transcription is a lack of training data---we derive a new dataset containing 50 hours of melody transcriptions from crowdsourced annotations of broad music. The combination of generative pre-training and a new dataset for this task results in 77% stronger performance on melody transcription relative to the strongest available baseline. By pairing our new melody transcription approach with solutions for beat detection, key estimation, and chord recognition, we build a system capable of transcribing human-readable lead sheets directly from music audio.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000058.pdf"
    },
    {
        "title": "Source Separation of Piano Concertos with Test-Time Adaptation",
        "author": [
            "Yigitcan Özer",
            "Meinard Müller"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "493-500",
        "abstract": "Music source separation (MSS) aims at decomposing a music recording into constituent sources, such as a lead instrument and the accompaniment. Despite the difficulties in MSS due to the high correlation of musical sources in time and frequency, deep neural networks (DNNs) have led to substantial improvements to accomplish this task. For training supervised machine learning models such as DNNs, isolated sources are required. In the case of popular music, one can exploit open-source datasets which involve multitrack recordings of vocals, bass, and drums. For western classical music, however, isolated sources are generally not available. In this article, we consider the case of piano concertos, which are composed for a pianist typically accompanied by an orchestra. The lack of multitrack recordings makes training supervised machine learning models for the separation of piano and orchestra challenging. To overcome this problem, we generate artificial training material by randomly mixing sections of the solo piano repertoire (e.g., piano sonatas) and orchestral pieces without piano (e.g., symphonies) to train state-of-the-art DNN models for MSS. As our main contribution, we propose a test-time adaptation (TTA) procedure, which exploits random mixtures of the piano-only and orchestra-only parts in the test data to further improve the separation quality.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000059.pdf"
    },
    {
        "title": "Counterpoint Error-Detection Tools for Optical Music Recognition of Renaissance Polyphonic Music",
        "author": [
            "Martha E Thomae Elias",
            "Julie Cumming",
            "Ichiro Fujinaga"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "501-508",
        "abstract": "This paper discusses part of a larger project to preserve and increase access to Guatemalan music sources written in mensural notation by using a digitization and music information retrieval (MIR) workflow to obtain both digital images and symbolic scores with editorial corrections. The workflow involves MIR tools such as optical music recognition (OMR), automatic voice alignment for mensural notation, editorial correction software, and computational counterpoint error detection. In this paper, we evaluate whether the use of automatic counterpoint error-detection tools makes the correction process more efficient. The results confirm that marking illegal dissonances in the score following the rules of Renaissance counterpoint indeed makes the process of editorial correction of scribal errors in Renaissance music more efficient by reducing the time taken and improving the accuracy of such corrections. Moreover, marking the illegal dissonances in the score also allowed us to catch OMR errors that had passed through undetected at a previous stage of the workflow.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000060.pdf"
    },
    {
        "title": "A Dataset of Symbolic Texture Annotations in Mozart Piano Sonatas",
        "author": [
            "Louis Couturier",
            "Louis Bigo",
            "Florence Leve"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "509-516",
        "abstract": "Musical scores are generally analyzed under different aspects, notably melody, harmony, rhythm, but also through their texture, although this last concept is arguably more delicate to formalize. Symbolic texture depicts how sounding components are organized in the score. It outlines the density of elements, their heterogeneity, role and interactions. In this paper, we release a set of manual annotations for each bar of 9 movements among early piano sonatas by W. A. Mozart, totaling 1164 labels that follow a syntax dedicated to piano score texture. A quantitative analysis of the annotations highlights some characteristic textural features in the corpus. In addition, we present and release the implementation of low-level descriptors of symbolic texture. These descriptors can be correlated with texture annotations and used in different machine-learning tasks. Along with provided data, they offer promising applications in computer assisted music analysis and composition.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000061.pdf"
    },
    {
        "title": "Violin Etudes: A Comprehensive Dataset for f0 Estimation and Performance Analysis",
        "author": [
            "Nazif Can Tamer",
            "Pedro Ramoneda",
            "Xavier Serra"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "517-524",
        "abstract": "Violin performance analysis requires accurate and robust f0 estimates to give feedback on the playing accuracy. Despite the recent advancements in data-driven f0 estimators, their application to performance analysis remains a challenge due to style-specific and dataset-induced biases. In this paper, we address this problem by introducing Violin Etudes, a 27.8-hours violin performance dataset constructed with domain knowledge in instrument pedagogy and a novel automatic f0-labeling paradigm. Experimental results on unseen datasets show that the CREPE f0 estimator trained on Violin Etudes outperforms the widely-used pre-trained version trained on multiple manually-labeled datasets. Further preliminary findings suggest that (i) existing data-driven f0 estimators may overfit to equal temperament, and (ii) iterative re-labeling regularized by our novel Constrained Harmonic Resynthesis method can simultaneously enhance datasets and f0 estimators. Our dataset curation methodology is easily scalable to other instruments owing to the quantity of pedagogical data online. It also supports a range of MIR research directions thanks to the performance difficulty labels from educational institutions.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000062.pdf"
    },
    {
        "title": "Checklist Models for Improved Output Fluency in Piano Fingering Prediction",
        "author": [
            "Nikita Srivatsan",
            "Taylor Berg-Kirkpatrick"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "525-531",
        "abstract": "In this work we present a new approach for the task of predicting fingerings for piano music. While prior neural approaches have often treated this as a sequence tagging problem with independent predictions, we put forward a checklist system, trained via reinforcement learning, that maintains a representation of recent predictions in addition to a hidden state, allowing it to learn soft constraints on output structure. We also demonstrate that by modifying input representations --- which in prior work using neural models have often taken the form of one-hot encodings over individual keys on the piano --- to encode relative position on the keyboard to the prior note instead, we can achieve much better performance. Additionally, we reassess the use of raw per-note labeling precision as an evaluation metric, noting that it does not adequately measure the fluency, i.e. human playability, of a model's output. To this end, we compare methods across several statistics which track the frequency of adjacent finger predictions that while independently reasonable would be physically challenging to perform in sequence, and implement a reinforcement learning strategy to minimize these as part of our training loss. Finally through human expert evaluation, we demonstrate significant gains in performability directly attributable to improvements with respect to these metrics.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000063.pdf"
    },
    {
        "title": "Sonus Texere! Automated Dense Soundtrack Construction for Books using Movie Adaptations",
        "author": [
            "Jaidev Shriram",
            "Makarand Tapaswi",
            "Vinoo Alluri"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "535-542",
        "abstract": "Reading, much like music listening, is an immersive experience that transports readers while taking them on an emotional journey. Listening to complementary music has the potential to amplify the reading experience, especially when the music is stylistically cohesive and emotionally relevant. In this paper, we propose the first fully automatic method to build a dense soundtrack for books, which can play high-quality instrumental music for the entirety of the reading duration. Our work employs a unique text processing and music weaving pipeline that determines the context and emotional composition of scenes in a chapter. This allows our method to identify and play relevant excerpts from the soundtrack of the book's movie adaptation. By relying on the movie composer's craftsmanship, our book soundtracks include expert-made motifs and other scene-specific musical characteristics. We validate the design decisions of our approach through a perceptual study. Our readers note that the book soundtrack greatly enhanced their reading experience, due to high immersiveness granted via uninterrupted and style-consistent music, and a heightened emotional state attained via high precision emotion and scene context recognition.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000064.pdf"
    },
    {
        "title": "Musika! Fast Infinite Waveform Music Generation",
        "author": [
            "Marco Pasini",
            "Jan Schlüter"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "543-550",
        "abstract": "Fast and user-controllable music generation could enable novel ways of composing or performing music. However, state-of-the-art music generation systems require large amounts of data and computational resources for training, and are slow at inference. This makes them impractical for real-time interactive use. In this work, we introduce Musika, a music generation system that can be trained on hundreds of hours of music using a single consumer GPU, and that allows for much faster than real-time generation of music of arbitrary length on a consumer CPU. We achieve this by first learning a compact invertible representation of spectrogram magnitudes and phases with adversarial autoencoders, then training a Generative Adversarial Network (GAN) on this representation for a particular music domain. A latent coordinate system enables generating arbitrarily long sequences of excerpts in parallel, while a global context vector allows the music to remain stylistically coherent through time. We perform quantitative evaluations to assess the quality of the generated samples and showcase options for user control in piano and techno music generation. We release the source code and pretrained autoencoder weights at github.com/marcoppasini/musika, such that a GAN can be trained on a new music domain with a single GPU in a matter of hours.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000065.pdf"
    },
    {
        "title": "Symphony Generation with Permutation Invariant Language Model",
        "author": [
            "Jiafeng Liu",
            "Yuanliang Dong",
            "Zehua Cheng",
            "Xinran Zhang",
            "Xiaobing Li",
            "Feng Yu",
            "Maosong Sun"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "551-558",
        "abstract": "In this work, we propose a permutation invariant language model, SymphonyNet, as a solution for symbolic symphony music generation. We propose a novel Multi-track Multi-instrument Repeatable (MMR) representation for symphonic music and model the music sequence using a Transformer-based auto-regressive language model with specific 3-D positional embedding. To overcome length overflow when modeling extra-long symphony tokens, we also propose a modified Byte Pair Encoding algorithm (Music BPE) for music tokens and introduce a novel linear transformer decoder architecture as a backbone. Meanwhile, we train the decoder to learn automatic orchestration as a joint task by masking instrument information from the input. We also introduce a large-scale symbolic symphony dataset for the advance of symphony generation research. Empirical results show that the proposed approach can generate coherent, novel, complex and harmonious symphony as a pioneer solution for multi-track multi-instrument symbolic music generation.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000066.pdf"
    },
    {
        "title": "MuLan: A Joint Embedding of Music Audio and Natural Language",
        "author": [
            "Qingqing Huang",
            "Aren Jansen",
            "Joonseok Lee",
            "Ravi Ganti",
            "Judith Yue Li",
            "Daniel P W Ellis"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "559-566",
        "abstract": "Music tagging and content-based retrieval systems have traditionally been constructed using pre-defined ontologies covering a rigid set of music attributes or text queries. This paper presents MuLan: a first attempt at a new generation of acoustic models that link music audio directly to unconstrained natural language music descriptions. MuLan takes the form of a two-tower, joint audio-text embedding model trained using 44 million music recordings (370K hours) and weakly-associated, free-form text annotations. Through its compatibility with a wide range of music genres and text styles (including conventional music tags), the resulting audio-text representation subsumes existing ontologies while graduating to true zero-shot functionalities. We demonstrate the versatility of the MuLan embeddings with a range of experiments including transfer learning, zero-shot music tagging, language understanding in the music domain, and cross-modal retrieval applications.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000067.pdf"
    },
    {
        "title": "MeloForm: Generating Melody with Musical Form based on Expert Systems and Neural Networks",
        "author": [
            "Peiling Lu",
            "Xu Tan",
            "Botao Yu",
            "Tao Qin",
            "Sheng Zhao",
            "Tie-Yan Liu"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "567-574",
        "abstract": "Human usually composes music by organizing elements according to the musical form to express music ideas. However, for neural network-based music generation, it is difficult to do so due to the lack of labelled data on musical form. In this paper, we develop MeloForm, a system that generates melody with musical form using expert systems and neural networks. Specifically, 1) we design an expert system to generate a melody by developing musical elements from motifs to phrases then to sections with repetitions and variations according to pre-given musical form; 2) considering the generated melody is lack of musical richness, we design a Transformer based refinement model to improve the melody without changing its musical form. MeloForm enjoys the advantages of precise musical form control by expert systems and musical richness learning via neural models. Both subjective and objective experimental evaluations demonstrate that MeloForm generates melodies with precise musical form control with 97.79% accuracy, and outperforms baseline systems in terms of subjective evaluation score by 0.75, 0.50, 0.86 and 0.89 in structure, thematic, richness and overall quality, without any labelled musical form data. Besides, MeloForm can support various kinds of forms, such as verse and chorus form, rondo form, variational form, sonata form, etc.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000068.pdf"
    },
    {
        "title": "Towards robust music source separation on loud commercial music",
        "author": [
            "Chang-Bin Jeon",
            "Kyogu Lee"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "575-582",
        "abstract": "Nowadays, commercial music has extreme loudness and heavily compressed dynamic range compared to the past. Yet, in music source separation, these characteristics have not been thoroughly considered, resulting in the domain mismatch between the laboratory and the real world. In this paper, we confirmed that this domain mismatch negatively affect the performance of the music source separation networks. To this end, we first created the out-of-domain evaluation datasets, musdb-L and XL, by mimicking the music mastering process. Then, we quantitatively verify that the performance of the state-of-the-art algorithms significantly deteriorated in our datasets. Lastly, we proposed LimitAug data augmentation method to reduce the domain mismatch, which utilizes an online limiter during the training data sampling process. We confirmed that it not only alleviates the performance degradation on our out-of-domain datasets, but also results in higher performance on in-domain data.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000069.pdf"
    },
    {
        "title": "Towards Quantifying the Strength of Music Scenes Using Live Event Data",
        "author": [
            "Michael Zhou",
            "Andrew Mcgraw",
            "Douglas R Turnbull"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "583-590",
        "abstract": "There are many benefits for a community when there is a vibrant local music scene (e.g., increased mental & physical well-being, increased economic activity) and there are many factors that contribute to an environment in which a live music scene can thrive (e.g., available performance spaces, helpful government policies). In this paper, we explore using an estimate of the live music event rate (LMER) as a rough indicator to measure the strength of a local music scene. We define LMER as the number of music shows per 100,000 people per year and then explore how this indicator is (or is not) correlated with 28 other socioeconomic indicators. To do this, we analyze a set of 308,051 music events from 2019 across 1,139 cities in the United States. Our findings reveal that factors related to transportation (e.g., high walkability), population (high density), economics (high employment rate), age (high proportion of individuals age 20-29), and education (bachelor’s degree or higher) are strongly correlated with having a high number of live music events. Conversely, we did not find statistically significant evidence that other indica- tors (e.g., racial diversity) are correlated.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000070.pdf"
    },
    {
        "title": "Learning Multi-Level Representations for Hierarchical Music Structure Analysis.",
        "author": [
            "Morgan Buisson",
            "Brian Mcfee",
            "Slim Essid",
            "Hélène C.  Crayencour Crayencour"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "591-597",
        "abstract": "Recent work in music structure analysis has shown the potential of deep features to highlight the underlying structure of music audio signals. Despite promising results achieved by such representations, dealing with the inherent hierarchical aspect of music structure remains a challenging problem. Because different levels of segmentation can be considered as equally valid, specifically designed representations should be optimized to improve hierarchical structure analysis. In this work, we explore unsupervised learning of such representations using a contrastive approach operating at different time-scales. We evaluate the proposed system on flat and multi-level music segmentation. By leveraging both time and the hierarchical organization of music structure, we show that the obtained deep embeddings can encode meaningful patterns and improve segmentation at various levels of granularity.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000071.pdf"
    },
    {
        "title": "Multi-instrument Music Synthesis with Spectrogram Diffusion",
        "author": [
            "Curtis Hawthorne",
            "Ian Simon",
            "Adam Roberts",
            "Neil Zeghidour",
            "Joshua Gardner",
            "Ethan Manilow",
            "Jesse Engel"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "598-607",
        "abstract": "An ideal music synthesizer should be both interactive and expressive, generating high-fidelity audio in realtime for arbitrary combinations of instruments and notes. Recent neural synthesizers have exhibited a tradeoff between domain-specific models that offer detailed control of only specific instruments, or raw waveform models that can train on any music but with minimal control and slow generation. In this work, we focus on a middle ground of neural synthesizers that can generate audio from MIDI sequences with arbitrary combinations of instruments in realtime. This enables training on a wide range of transcription datasets with a single model, which in turn offers note-level control of composition and instrumentation across a wide range of instruments. We use a simple two-stage process: MIDI to spectrograms with an encoder-decoder Transformer, then spectrograms to audio with a generative adversarial network (GAN) spectrogram inverter. We compare training the decoder as an autoregressive model and as a Denoising Diffusion Probabilistic Model (DDPM) and find that the DDPM approach is superior both qualitatively and as measured by audio reconstruction and Fréchet distance metrics. Given the interactivity and generality of this approach, we find this to be a promising first step towards interactive and expressive neural synthesis for arbitrary combinations of instruments and notes.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000072.pdf"
    },
    {
        "title": "DDX7: Differentiable FM Synthesis of Musical Instrument Sounds",
        "author": [
            "Franco Caspe",
            "Andrew Mcpherson",
            "Mark Sandler"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "608-616",
        "abstract": "FM Synthesis is a well-known algorithm used to generate complex timbre from a compact set of design primitives. Typically featuring a MIDI interface, it is usually impractical to control it from an audio source. On the other hand, Differentiable Digital Signal Processing (DDSP) has enabled nuanced audio rendering by Deep Neural Networks (DNNs) that learn to control differentiable synthesis layers from arbitrary sound inputs. The training process involves a corpus of audio for supervision, and spectral reconstruction loss functions. Such functions, while being great to match spectral amplitudes, present a lack of pitch direction which can hinder the joint optimization of the parameters of FM synthesizers. In this paper, we take steps towards enabling continuous control of a well-established FM synthesis architecture from an audio input. Firstly, we discuss a set of design constraints that ease spectral optimization of a differentiable FM synthesizer via a standard reconstruction loss. Next, we present Differentiable DX7 (DDX7), a lightweight architecture for neural FM resynthesis of musical instrument sounds in terms of a compact set of parameters. We train the model on instrument samples extracted from the URMP dataset, and quantitatively demonstrate its comparable audio quality against selected benchmarks.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000073.pdf"
    },
    {
        "title": "Singing beat tracking with Self-supervised front-end and linear transformers",
        "author": [
            "Mojtaba Heydari",
            "Zhiyao Duan"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "617-624",
        "abstract": "Tracking beats of singing voices without the presence of musical accompaniment can find many applications in music production, automatic song arrangement, and social media interaction. Its main challenge is the lack of strong rhythmic and harmonic patterns that are important for music rhythmic analysis in general. Even for human listeners, this can be a challenging task. As a result, existing music beat tracking systems fail to deliver satisfactory performance on singing voices. In this paper, we propose singing beat tracking as a novel task, and propose the first approach to solving this task. Our approach leverages semantic information of singing voices by employing pre-trained self-supervised WavLM and DistilHuBERT speech representations as the front-end and uses a self-attention encoder layer to predict beats. To train and test the system, we obtain separated singing voices and their beat annotations using source separation and beat tracking on complete songs, followed by manual corrections. Experiments on the 741 separated vocal tracks of the GTZAN dataset show that the proposed system outperforms several state-of-the-art music beat tracking methods by a large margin in terms of beat tracking accuracy. Ablation studies also confirm the advantages of pre-trained self-supervised speech representations over generic spectral features.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000074.pdf"
    },
    {
        "title": "EnsembleSet: a new high quality synthesised dataset for chamber ensemble separation",
        "author": [
            "Saurjya Sarkar",
            "Emmanouil Benetos",
            "Mark Sandler"
        ],
        "year": "2022",
        "doi": null,
        "url": null,
        "video_url": null,
        "poster_url": null,
        "thumbnail_url": null,
        "pages": "625-632",
        "abstract": "Music source separation research has made great advances in recent years, especially towards the problem of separating vocals, drums, and bass stems from mastered songs. The advances in this field can be directly attributed to the availability of large-scale multitrack research datasets for these mentioned stems. Tasks such as separating similar-sounding sources from an ensemble recording have seen limited research due to the lack of sizeable, bleed-free multitrack datasets. In this paper, we introduce a novel multitrack dataset called EnsembleSet generated using the Spitfire BBC Symphony Orchestra library using ensemble scores from RWC Classical Music Database and Mutopia. Our data generation method introduces automated articulation mapping for different playing styles based on the input MIDI/MusicXML data. The sample library also enables us to render the dataset with 20 different mix/microphone configurations allowing us to study various recording scenarios for each performance. The dataset presents 80 tracks (6+ hours) with a range of string, wind, and brass instruments arranged as chamber ensembles. We also present our benchmark on our synthesised dataset using a permutation-invariant time-domain separation model for chamber ensembles which produces generalisable results when tested on real recordings from existing datasets.",
        "zenodo_id": null,
        "dblp_key": null,
        "ee": "../temp_data/split_articles/000075.pdf"
    }
]
